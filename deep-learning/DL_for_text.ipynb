{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBMTxQf/9iBVOHjKt/Mn5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stubberson/project-collection/blob/main/deep-learning/DL_for_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural language processing\n",
        "This notebook follows Francois Chollet's book [Deep Learning with Python (2021)](https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf). This is chapter 11 of the book.\n",
        "\n",
        "\"Natural\" languages refer to human languages, like English or Finnish, to distinguish them from languages that were designed for machines, like XML or Assembly. Machine languages are _designed_: a set of formal rules – the syntax – is described by an engineer. Machine languages are extremely exact and rigorous, meanings cannot be interpreted. With human language, it's the reverse: usage comes first, rules arise later. It is messy, ambiguous and in a constant flux.\n",
        "\n",
        "Creating algorithms that can make sense of natural language is a big deal: language, and in particular text, underpins most of our communications and our cultural production.\n",
        "\n",
        "Modern natural language processing (NLP) is about using machine learnign and large datasets to give computers the ability not to _understand_ language, which is a more lofty goal, but to ingest a piece of language as input and return something useful, like predicting the following:\n",
        "- _Text classification_ – The topic of a text\n",
        "- _Content filtering_ – Does this text contain abuse?\n",
        "- _Sentiment analysis_ – Is a text positive or negative?\n",
        "- _Language modelling_ – What should be the next word in this incomplete sentence?\n",
        "- _Translation_ – How would you say this in German?\n",
        "- _Summarization_ – How would you summarize this article?\n",
        "\n",
        "Text-processing models won't possess a human-like understanding of language; rather, they simply look for statistical regularities in their input data, which turns out to be sufficient to perform well on many simple tasks. In much the same way that commputer \"vision\" is pattern recognition applied to pixels, NLP is pattern recognition applied to words, sentences, and paragraphs."
      ],
      "metadata": {
        "id": "DqyUKtReK-eE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing text data\n",
        "Deep learning models, being differentiable functions, can only process numeric tensors: they can't take raw text as input. _Vectorizing_ text is the process of transforming text into numeric tensors:\n",
        "1. First, you _standardize_ the text to make it easier to process, such as by converting it to lowercase or removing punctuation.\n",
        "2. You split the text into _tokens_, such as characters, words, or groups of words. This is called _tokenization_\n",
        "    - _Word-level tokenization_ separates tokens by a specific character (e.g. space or punctuation). This is used with a _sequence model_.\n",
        "    - _N-gram tokenization_ creates tokens of _N_ consecutive words. \"The cat\" would be a 2-gram token. This is used with a _bag-of-words model_.\n",
        "3. You convert each such token into a numerical vector. This usually involves _indexing_ all tokens present in the data.\n",
        "```python\n",
        "\"\"\" An example of the vectorization process without any Keras or Tensorflow functionality \"\"\"\n",
        "import string\n",
        "'\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text\n",
        "            if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[OOV]\": 1}  # The 0-index is usually reserved for mask token and the 1-index for the OOV token\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "                    self.inverse_vocabulary = dict(\n",
        "                        (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "```"
      ],
      "metadata": {
        "id": "DSF_QPaQPpo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, however, we'd use the Kears `TextVectorization` layer, which is fast and efficient and can be dropped directly into a `tf.data` pipeline on a Keras model. This is what the layer looks like:\n",
        "```python\n",
        "text_vectorization = TextVectorization(output_mode=\"int\",)\n",
        "```\n",
        "The layer returns sequences of words encoded as integer indices. There are also other output models available. By default, the layer uses the setting \"convert to lowercase and remove punctuation\" for _standardization_, and \"split on whitespace\" for _tokenization_."
      ],
      "metadata": {
        "id": "kTGDjevJTZok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two approaches for representing groups of words: sets and sequences\n",
        "_Word order_ is a fundamental problem for machine learning: unlike the steps of a timeseries, words in a sentence don't have a natural, canonical order. Different languages order similar words in very different ways. Order is important, but its relationship to meaning isn't straightforward.\n",
        "\n",
        "The simplest thing you could do is to just discard order and treat text as an unordered set of words – the _bag-of-words models_ – or you could treat words as an ordered sequence – here an RNN would be the choice. Additionally, a hybrid approach is possible: the **Transformer architecture**.\n",
        "\n",
        "Transformers are order-agnostic, yet they inject word-position information into the representations it processes, which enables it to simultaneously look at different parts of a sentence (unlike RNNs) while still being order-aware. Because they take into account word order, both RNNs and Transfromers are called _sequence models_."
      ],
      "metadata": {
        "id": "NTARx3btnzyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMDB example\n",
        "Let's demonstrate both approaches with the same IMDB sentiment-classification dataset as previously. First, we'll prepare the IMDB movie reviews data – this time from scratch."
      ],
      "metadata": {
        "id": "17DMf_y4rUz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zME0Nng2rzZa",
        "outputId": "8cced84d-3a07-4047-8bf5-6a7d93e2d21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  5610k      0  0:00:14  0:00:14 --:--:-- 13.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the content of a few of these text files\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie2_js5YsiYV",
        "outputId": "9ec41be8-213e-4bc8-9490-3733d87ea387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's prepare a validation set by setting apart 20% of the training text files in a new directory, `aclImdb/val`:"
      ],
      "metadata": {
        "id": "nU9M-_5LvyY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os, pathlib, shutil, random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "mee-egbnz9Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))  # Take 20% of the training for val\n",
        "    val_files = files[-num_val_samples:]  # and slice it from the end of files\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,  # Move the files from train to val\n",
        "                    val_dir / category / fname)"
      ],
      "metadata": {
        "id": "MiF-L_mdv-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a batched `Dataset` by using the `text_dataset_from_directory` utility."
      ],
      "metadata": {
        "id": "R1WpHeVxzXph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETS1rI6Rzkbu",
        "outputId": "20b86f69-0f28-4bcf-89e9-98167e7ee849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What kind of data do we have?\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"Inputs shape:\", inputs.shape)\n",
        "    print(\"Inputs data type:\", inputs.dtype)\n",
        "    print(\"Targets shape:\", targets.shape)\n",
        "    print(\"Targets data type:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2BkE3uQ0bCS",
        "outputId": "4cbb5479-0038-4e87-f02a-f306537bea9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs shape: (32,)\n",
            "Inputs data type: <dtype: 'string'>\n",
            "Targets shape: (32,)\n",
            "Targets data type: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'All day now I\\'ve been watching dinosaurs, and all day they\\'ve had the same fundamental problem.<br /><br />They don\\'t believe in firearms. They just don\\'t seem to have been _told_ about them or something. Bullets _bounce_ off of dinosaurs! Maybe it\\'s because they became extinct millions of years before the invention of gunpowder, and the laws of physics were just different back then... Aah, no. Come on. If they\\'re close enough to chemically operate today, they\\'d have to be vulnerable to fast (even subsonic) lead projectiles. It\\'s that simple.<br /><br />Look, the toughest-skinned reptiles on the planet today, alligators and crocodiles, are completely vulnerable to basic rifle fire. They\\'re nothing magic. You can shoot a pistol round right through the heavy scales on their backs. They don\\'t take armor-piercing bullets or anything special. Small bullets penetrate them, they just don\\'t kill them. Somewhat (but not REALLY) large bullets are preferred because the challenge (as with most game) is to kill the animal with one shot, so it doesn\\'t run. (Hunters consider it immoral to allow prey to run off and die unharvested.)<br /><br />Most animals, including predators, are easily repelled by gunfire. Between the noise, and the pain of even a non-lethal wound, most will run away. An exception are big bears, which are so fearless that they\\'re merely enraged by mortal wounds. Cape buffalo are regarded as highly dangerous because they are well known to charge when wounded. We\\'ve seen video of the big bulls of a herd of cape buffalo rescuing a calf from an entire pride of lions. A big cat will run if it can, but if it can\\'t it will charge as a final act of desperation. Where a T.Rex would fit in this spectrum is unknown. Their behavior simply has not been observed. With these larger animals, safe hunting becomes a matter of applying an appropriately large and powerful projectile, and/or applying several of them rapidly enough to counter its charge. With a T.Rex, of course, this could be a serious problem. I\\'ve seen a T.Rex skull (they have one in the museum downtown) and carrying a gun big enough to bust that might be impractical. Chewing its neck off with lots of smaller fire might be a more viable approach. Small bullets would still _penetrate_ them, they wouldn\\'t just bounce off just because the animal is too big to easily kill! <br /><br />So here we have Cortez and his men (this is _before_ the famous Mexican campaign, apparently) captured by American natives and scheduled for sacrifice on the pyramid. It appears that all those human sacrifices were about appeasing the bloodthirst of the pair of T.Rexes that terrorized the continent in the day. Rather than just having their hearts cut out and being fed to the lizards, Cortez et al talk the Aztecs into letting them hunt & kill them. OK, maybe they don\\'t have M-16s like the guys in the \"Carnosaur\" series, but they _do_ have flintlocks, crossbows, pointed sticks (big ones, made from trees) and swords. Maybe that\\'s a little less uneven than squads of soldiers with full auto, but they\\'ve several guys and I\\'d quickly bet on them over a dinosaur. Oh, wait, there\\'s a _cannon_, about a 4-incher. That\\'s just the ticket for busting a Tyrannosaurus\\' skull! So they lay a trap, with a squad of men, cannon, pointed sticks in a ravine, and lure the first T.Rex into it, using a pretty brown girl as bait. Cortez points out that they\\'ll NOT have time to reload, so they\\'ll have to close the range until they can be certain of their aim. T.Rex totally ignores their volley of flintlock fire, and we see both a crossbow bolt _and_ the cannon ball _bounce_ off! Forget it. End of credibility. A crossbow bolt would defeat Cortez\\' torso armor, and a 4\" cannon ball might penetrate the hull of a wooden ship! This would also _certainly_ get through the hide, ribcage, or skull of any animal ever to walk this planet. (Do you think a _whale_ could withstand a 4\" cannon ball?) And here\\'s T.Rex, still standing, not even bleeding. So Cortez lures it to the ravine, where it falls onto the pointed sticks, which (I guess by magic) penetrate it and kill it. Yaaay, pointed sticks! <br /><br />The dinos aren\\'t completely invulnerable to gunfire - they manage to put out an eye of the second one with a pistol. This runs it off, so it\\'s NOT as mean as a bear or a buffalo, at least in the movies.<br /><br />They kill the second dinosaur with a bomb - made from a gourd filled with gunpowder and gemstones. My money would still be on the cannon. It\\'s engineered function is to concentrate all the gunpowder\\'s energy in one direction - toward the target. A bomb is a much more diffused application of force. A _real_ bomb (NOT a gourd bomb) has a steel casing which contains the explosion to extremely high pressure. (Think: pipe bomb vs firecracker.) A pile of gunpowder set on fire will simply go POOF. (Trust me on that one.)', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Processing words as a set: the bag-of-words approach\n",
        "The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set of tokens.\n",
        "\n",
        "**SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING**\n",
        "\n",
        "If you use a bag of single words, the sentence \"the cat sat on the mat\" becomes `{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}`\n",
        "\n",
        "The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance, using binary encoding (multi-hot), you'd encode a text as a vector with as many dimensions as there are words in your vocabulary – with 0s almost everywhere and some 1s for dimensions that encode words present in the text. Let's try this on our task.\n",
        "\n",
        "First, let's process our raw text datasets with a `TextVectorization` layer so that they yield multi-hot encoded binary word vectors. Our layer will only look at single words (i.e., _unigrams_)."
      ],
      "metadata": {
        "id": "cNSujT3K2pLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=20000,  # Limit the vocabulary to the 20,000 most frequent words\n",
        "    output_mode=\"multi_hot\",  # Encode as multi-hot binary vectors\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)  # A dataset w/ only raw text\n",
        "text_vectorization.adapt(text_only_train_ds)  # Index with the adapt method\n",
        "\n",
        "# Prepare processed versions of the train, val, and test datasets\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4  # Leverage multiple CPU cores\n",
        ")\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4\n",
        ")\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "tMtV5dGQ4nC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What do we have?\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ez0CLU57BjS",
        "outputId": "2b42f858-fe29-4a46-ab77-d5f009acd5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'int64'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1 1 0 ... 0 0 0], shape=(20000,), dtype=int64)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's write a reusable model-building function\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "G9Z1sxl3-vdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's then train and test our model."
      ],
      "metadata": {
        "id": "tR2bEhO8_6_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model and check its summary\n",
        "model = get_model()\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "PfkQatoS_9-k",
        "outputId": "7c171232-a422-4b24-b900-f397005f5f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │         \u001b[38;5;34m320,016\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.7747 - loss: 0.4836 - val_accuracy: 0.8902 - val_loss: 0.2768\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8983 - loss: 0.2775 - val_accuracy: 0.8956 - val_loss: 0.2731\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9155 - loss: 0.2380 - val_accuracy: 0.8966 - val_loss: 0.2878\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9226 - loss: 0.2214 - val_accuracy: 0.8948 - val_loss: 0.3005\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9296 - loss: 0.2115 - val_accuracy: 0.8918 - val_loss: 0.3191\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9335 - loss: 0.2075 - val_accuracy: 0.8856 - val_loss: 0.3386\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9345 - loss: 0.2027 - val_accuracy: 0.8902 - val_loss: 0.3467\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9314 - loss: 0.2027 - val_accuracy: 0.8882 - val_loss: 0.3491\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9341 - loss: 0.2017 - val_accuracy: 0.8914 - val_loss: 0.3531\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9379 - loss: 0.1995 - val_accuracy: 0.8832 - val_loss: 0.3668\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8859 - loss: 0.2855\n",
            "Test acc: 0.884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a test accyracy of 88.8%. In this case, since the dataset is a balanced two-class classification dataset (_balanced_, there are as many positive and as there are negative samples), the \"naive baseline\" we could reach without training an actual model would only be 50%.\n",
        "\n",
        "**BIGRAMS WITH BINARY ENCODING**\n",
        "\n",
        "Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words: the term \"United States\" conveys a concept that is quite distinct from the meaning of the words \"states\" and \"united\" taken separately. For this reason, you will usually end up re-injecting local order information into your bag-of-words representation by looking at N-grams rather than single words.\n",
        "\n",
        "With bigrams, our sentence \"the cat sat on the mat\" becomes:\n",
        "`{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}`"
      ],
      "metadata": {
        "id": "zZ5c1nUBBMEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the TextVectorization layer to return bigrams\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\"\n",
        ")"
      ],
      "metadata": {
        "id": "MvaZpelPEvCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the binary bigram model\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "518dL0O4FKMs",
        "outputId": "19148123-c880-453b-91d3-41fd10edb2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │         \u001b[38;5;34m320,016\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.7806 - loss: 0.4635 - val_accuracy: 0.9004 - val_loss: 0.2598\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9112 - loss: 0.2489 - val_accuracy: 0.9026 - val_loss: 0.2731\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9273 - loss: 0.2126 - val_accuracy: 0.9038 - val_loss: 0.2810\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9375 - loss: 0.1855 - val_accuracy: 0.9090 - val_loss: 0.2942\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9437 - loss: 0.1849 - val_accuracy: 0.9080 - val_loss: 0.3085\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9500 - loss: 0.1772 - val_accuracy: 0.9034 - val_loss: 0.3303\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9538 - loss: 0.1765 - val_accuracy: 0.9004 - val_loss: 0.3456\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9530 - loss: 0.1666 - val_accuracy: 0.9002 - val_loss: 0.3517\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9548 - loss: 0.1678 - val_accuracy: 0.8968 - val_loss: 0.3621\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9542 - loss: 0.1617 - val_accuracy: 0.8988 - val_loss: 0.3699\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8988 - loss: 0.2647\n",
            "Test acc: 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do get an improvement! Local order seems to be important in sentiment analysis.\n",
        "\n",
        "**BIGRAMS WITH TF-IDF ENCODING**\n",
        "\n",
        "You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text:\n",
        "\n",
        "`{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1, \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}`\n",
        "\n",
        "In text classification, knowing how many times a word occurs in a sample is critical: any sufficiently long movie review may contain the word \"terrible\" regardless of sentiment, but a review that contains many instances of that word is likely a negative one."
      ],
      "metadata": {
        "id": "YoakBN00GNF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's configure the TextVectorization layer for returning token counts\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "erMw1gBgHR55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, of course, some words are bound to occur more often than other no matter what the text is about. The words \"the\", \"a\", \"is\", and \"are\" will always dominate your word counts, drowning out other words – despite being pretty much useless features in a classification context.\n",
        "\n",
        "This can be addressed by _normalization_. Most vectorized sentences consist almost entirely of zeros (the previous example features 12 non-zero entries and 19,988 zero entries), a property called _sparsity_. That's a great property to have, as it dramatically reduces compute load and reduces the risk of overfitting. This is why the basic _z-score_ normalization isn't such a good idea (our zeros would vanish because of the subtraction by the mean).\n",
        "\n",
        "_TF-IDF normalization_ (term frequency, inverse document frequency) is used to combat this problem. The more a given term appears in a document, the more important that term is for understanding what the document is about. At the same time, the frequency at which the term appears across all documents in your dataset matters too: terms that appear in almost every document (like “the” or “a”) aren't particularly informative, while terms that appear only in a small subset of all texts (like “Herzog”) are very distinctive, and thus important. TF-IDF is a metric that fuses these two ideas."
      ],
      "metadata": {
        "id": "SnDtXM0LHqa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's configure the TextVectorization layer for returning TF-IDF counts\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZEVJ0tw0JhfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the binary bigram model\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "SoPpACdaJrnf",
        "outputId": "3c2b53fa-426d-44d9-bc12-9e95545f708f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │         \u001b[38;5;34m320,016\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.6986 - loss: 0.7846 - val_accuracy: 0.8896 - val_loss: 0.3058\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8431 - loss: 0.3540 - val_accuracy: 0.8914 - val_loss: 0.3094\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8619 - loss: 0.3127 - val_accuracy: 0.8938 - val_loss: 0.3045\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8857 - loss: 0.2734 - val_accuracy: 0.8980 - val_loss: 0.2990\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8921 - loss: 0.2657 - val_accuracy: 0.8832 - val_loss: 0.3188\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8970 - loss: 0.2578 - val_accuracy: 0.8800 - val_loss: 0.3668\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9013 - loss: 0.2429 - val_accuracy: 0.8962 - val_loss: 0.3396\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9070 - loss: 0.2361 - val_accuracy: 0.8658 - val_loss: 0.3516\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9105 - loss: 0.2206 - val_accuracy: 0.8848 - val_loss: 0.3438\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9144 - loss: 0.2196 - val_accuracy: 0.8810 - val_loss: 0.3512\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8941 - loss: 0.3109\n",
            "Test acc: 0.895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doesn't improve our test accuracy. However, for many text-classification datasets, it would be typical to see a one-percentage-point increase when using TF-IDF compared to plain binary encoding."
      ],
      "metadata": {
        "id": "JKyuDzvbKmAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Processing words as a sequence: the sequence model approach\n",
        "The previous examples show that order does matter in language processing. Until now, we have hand-crafted our sequences that the model ought to understand as sequences, but a better way is to let the algorithm do this on its own.\n",
        "\n",
        "To implement a _sequence model_:\n",
        "1. Start by representing your input samples as sequences of integer indices (one integer standing for one word).\n",
        "2. Map each integer to a vector to obtain vector sequences.\n",
        "3. Finally, feed these sequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convent, a RNN, or a Transformer.\n",
        "\n",
        "**First practical example:**\n",
        "\n",
        "First, we need to prepare datasets that return integer sequences."
      ],
      "metadata": {
        "id": "CM23ls_vK2fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600  # We'll truncate the the inputs after the first 600 words\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "1ld01Xh8ShrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's make a model. The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers. On top of these one-hot vectors, we'll add a simple bidirectional LSTM."
      ],
      "metadata": {
        "id": "_cyjoCBhTpn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "# This is different from the book, but should do the exact same thing\n",
        "embedded = keras.ops.one_hot(inputs, num_classes=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "DzomF58JT32V",
        "outputId": "ba3fea4f-2482-4c19-dd27-a598de988016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ one_hot (\u001b[38;5;33mOneHot\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │       \u001b[38;5;34m5,128,448\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ one_hot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OneHot</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,128,448</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,128,513\u001b[0m (19.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,128,513</span> (19.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,128,513\u001b[0m (19.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,128,513</span> (19.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HOX! VERY SLOW, don't do again!\n",
        "# Then train our model\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds,\n",
        "          validation_data=int_val_ds,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCJh8bCVaF-D",
        "outputId": "8e386759-2224-45cb-9b1b-eb3af39f1b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 363ms/step - accuracy: 0.6102 - loss: 0.6432 - val_accuracy: 0.8546 - val_loss: 0.3668\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 363ms/step - accuracy: 0.8495 - loss: 0.3882 - val_accuracy: 0.8820 - val_loss: 0.3095\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 389ms/step - accuracy: 0.8881 - loss: 0.3165 - val_accuracy: 0.8614 - val_loss: 0.3368\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 363ms/step - accuracy: 0.9090 - loss: 0.2647 - val_accuracy: 0.8940 - val_loss: 0.3232\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 388ms/step - accuracy: 0.9229 - loss: 0.2295 - val_accuracy: 0.8956 - val_loss: 0.3002\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 389ms/step - accuracy: 0.9308 - loss: 0.2065 - val_accuracy: 0.8930 - val_loss: 0.3174\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 389ms/step - accuracy: 0.9400 - loss: 0.1800 - val_accuracy: 0.8886 - val_loss: 0.3145\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 389ms/step - accuracy: 0.9474 - loss: 0.1680 - val_accuracy: 0.8888 - val_loss: 0.3437\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 389ms/step - accuracy: 0.9521 - loss: 0.1431 - val_accuracy: 0.8870 - val_loss: 0.3584\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 389ms/step - accuracy: 0.9605 - loss: 0.1232 - val_accuracy: 0.8726 - val_loss: 0.3410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a94f81f5290>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNucLgwKaY6H",
        "outputId": "4683c575-d560-46ad-de41-108c57b93737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 161ms/step - accuracy: 0.8685 - loss: 0.3376\n",
            "Test acc: 0.868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A first observation: this model is extremely slow to train when compared to the previous models. This is because our inputs are quite large: each input sample is encoded as a matrix of size `(600, 20000)` (600 words per sample, 20,000 possible words). That's 12,000,000 floats for a single movie review. Second, the model only gets to 87% test accuracy, not as good as the simpler models.\n",
        "\n",
        "Clearly, using one-hot encoding to turn words into vectors, which was the simplest thing we could do, wasn't a great idea. There's a better way: _word embeddings_.\n",
        "\n",
        "**UNDERSTANDING WORD EMBEDDINGS**\n",
        "\n",
        "Crucially, when encoding something via one-hot encoding, you're making a feature-engineering decision. You're injecting into your model a fundamental assumption about the structure of your feature space. That assumption is that _the different tokens you're encoding are all independent from each other_: indeed, one-hot vectors are all orthogonal to each other. And in the case of words, that assumption is clearly wrong. Words form a structured space: they share information with each other. The words \"movie\" and \"film\" are interchangeable in most sentences, so the vector that represents \"movie\" should not be orthogonal to the vector that represents \"film\" – they should be the same vector, or close enough.\n",
        "\n",
        "The _geometric relationship_ between two vectors should reflect the _semantic relationship_ between these words. Words that mean different things should lie far away from each other in the geometric space.\n",
        "\n",
        "> _Word embeddings_ are vector representations of words that achieve exactly this: they map human language into a structured geometric space.\n",
        "\n",
        "Whereas the vectors obtained through one-hot encoding are binary, sparse, and very high-dimensional, word embeddings are dense, low-dimensional floating-point vectors. So, word embeddings pack more information into far fewer dimensions when compared to one-hot encoding.\n",
        "\n",
        "Besides being _dense_ representations, word embeddings are also _structured_ representations, and their structure is learned from data. Similar words get embedded in. close locations, and further, specific _directions_ in the embedding space are meaningful.\n",
        "\n",
        "As an example, we could have the words _cat_, _dog_, _wolf_, and _tiger_. With the vector representations we chose here, some semantic relationships between these words can be encoded as geometric transformations. For instance, the same vector allows us to go from _cat_ to _tiger_ and from _dog_ to _wolf_: this vector could be interpreted as the \"from pet to wild animal\" vector.\n",
        "\n",
        "In general, there are two ways to obtain word embeddings:\n",
        "- Learn word embeddings jointly with the main task you care about. In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
        "- Load into your model word embeddings that were precomputed using a different machine learning task than the one you're trying to solve. These are called _pretrained word embeddings_.\n",
        "\n",
        "**LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER**\n",
        "\n",
        "Usually you _learn_ a new embedding space with every new task. In Keras this is easy: we use the `Embedding` layer."
      ],
      "metadata": {
        "id": "k1tIy8V-bJKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "metadata": {
        "id": "xwenAtELqWlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Embedding` layer is best understood as a dictionary that maps integer indices to dense vectors. It takes integers as input, looks up these integers in an internal dictionary, and returns the associated vectors.\n",
        "\n",
        "Let's build a model that includes an `Embedding` layer and benchmark it on our task."
      ],
      "metadata": {
        "id": "Owfi_tISqo0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds,\n",
        "          validation_data=int_val_ds,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "QkK6wV_PrhR6",
        "outputId": "c9639c2f-9dfa-485c-a113-805b24f56759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m5,120,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m73,984\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,984</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,194,049\u001b[0m (19.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,194,049</span> (19.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,194,049\u001b[0m (19.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,194,049</span> (19.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.6392 - loss: 0.6218 - val_accuracy: 0.8132 - val_loss: 0.4372\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 46ms/step - accuracy: 0.8302 - loss: 0.4241 - val_accuracy: 0.8288 - val_loss: 0.3960\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 47ms/step - accuracy: 0.8763 - loss: 0.3325 - val_accuracy: 0.8796 - val_loss: 0.3131\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - accuracy: 0.8983 - loss: 0.2738 - val_accuracy: 0.8522 - val_loss: 0.4266\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.9149 - loss: 0.2409 - val_accuracy: 0.8888 - val_loss: 0.3210\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9297 - loss: 0.2070 - val_accuracy: 0.8822 - val_loss: 0.3557\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.9413 - loss: 0.1803 - val_accuracy: 0.8872 - val_loss: 0.3279\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.9501 - loss: 0.1488 - val_accuracy: 0.8808 - val_loss: 0.4073\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 47ms/step - accuracy: 0.9597 - loss: 0.1230 - val_accuracy: 0.8910 - val_loss: 0.4040\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9698 - loss: 0.0959 - val_accuracy: 0.8764 - val_loss: 0.4316\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8700 - loss: 0.3271\n",
            "Test acc: 0.869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains a lot faster than the one-hot model (because the LSTM layer only has to process 256-dimensional instead of 20,000 vectors), and its test accuracy is comparable. However, we're still some way off from the results of our basic bigram model. Part of the reason why is sipmly that the model is looking at slightly less data: the bigram model processed full reviews, while our sequence model truncates sequences after 600 words.\n",
        "\n",
        "**UNDERSTANDING PADDING AND MASKING**\n",
        "\n",
        "One thing that's slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of `output_sequence_length=max_length` option in `TextVectorization`: sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter than 600 tokens are padded with zeros at the end so that they can be concatenated together with other sequences to form contiguous batches.\n",
        "\n",
        "We're using a bidirectional RNN: two RNN layers running in parallel, with one processing the tokens in their natural order, and the other processing the same tokens in reverse. The RNN that looks at the tokens in their natural order will spend its alst iterations seeing only vectors that encode padding – possibly for hundreds of iterations if the original sentence was short. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\n",
        "\n",
        "We need a way to tell the RNN that it should skip these iterations: in steps _masking_. The mask is a tensor of 1s and 0s, where all 0s in the original input are represented as 0s and all other data as 1s.\n",
        "\n",
        "Masking can be enabled in an `Embedding` layer with `mask_zero=True`.\n",
        "\n",
        "**USING PRETRAINED EMBEDDINGS**\n",
        "\n",
        "Sometimes there's very little training data available and you cannot use your data alone to learn an appropriate task-specific embedding of your vocabulary. For such cases precomputed embedding vectors are useful.\n",
        "\n",
        "One of the most famous word-embedding schemes is the `word2vec` algorithm. Its dimensions capture specific semantic properties, such as gender."
      ],
      "metadata": {
        "id": "mmdoi9E3tHgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Transformer architecture\n",
        "Starting in 2017, a new model architecture started overtaking recurrent neural networks across most natural language processing tasks: the Transformer.\n",
        "\n",
        "A simple mechanism called \"_neural attention_\" could be used to build powerful sequence models that didn't feature anay recurrent layers or convolution layers.\n",
        "\n",
        "This finding unleashed a revolution in NLP and beyond. Neural attention has fast become one of the most influential ideas in deep learning."
      ],
      "metadata": {
        "id": "igOiaebP0Lzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding self-attention\n",
        "It's a simple yet powerful idea: not all input information seen by a model is equally important to the task at hand, so models should \"pay more attention\" to some features and \"pay less attention\" to others.\n",
        "\n",
        "It is a similar idea to `MaxPooling` in convnets that looks at a pool of features in a spatial region and selects just one feature to keep. Additionally, `TF-IDF` normalization assigns importance scores to tokens based on how much information different tokens are likely to carry. Important tokens get boosted while irrelevant tokens get faded out.\n",
        "\n",
        "Crucially, this kind of attention mechanism can be used for more than just highlighting or erasing certain features. It can be used to make features _context-aware_. You've just learned about word embeddings – vector spaces that capture the \"shape\" of the semantic relationships between different words. In an embedding space, a single word has a fixed position – a fixed set of relationships with every other word in the space. But that's not quite how language works: the meaning of a word is usually context-specific. When you mark the date, you're not talking about the same \"date\" as when you go on a date, nor is it the kind of date you'd buy at the market. When you say, \"I'll see you soon,\" the meaning of the word \"see\" is subtly different from the \"see\" in \"I'll see this project to its end,\" or \"I see what you mean.\"\n",
        "\n",
        "Clearly, a smart embedding space would provide a different vector representation for a word depending on the other words surrounding it. That's where _self-attention_ comes in. The purpose of self-attention is to modulate the representation of a token by using the representations of related tokens in the sequence. This produces context-aware token representations. Consider an example sentence: \"The train left the station on time.\" Now, consider one word in the sentence: station. What kind of station are we talking about? Could it be a radio station? Maybe a the International Space Station? Let's figure out algorithmically via self-attention.\n",
        "\n",
        "- **Step 1** is to compute relevancy scores between the vector for \"station\" and every other word in the sentence. These are our \"attention scores.\" We're simply going to use the dot product between two word vectors as a measure of the strength of their relationship.\n",
        "- **Step 2** is to compute the sum of all word vectors in the sentence, weighted by our relevancy scores. Words closely related to \"station\" will contribute more to the sum, while irrelevant words will contribute almost nothing. The resulting vector is our new representation for \"station\": a representation that incorporates the surrounding context. In particular, it includes part of the \"train\" vector, clarifying that it is, in fact, a \"train station\".\n",
        "\n",
        "This process would be then repeated for every word in the sentence, producing a new sequence of vectors encoding the sentence.\n",
        "\n",
        "```python\n",
        "\"\"\" Pseudocode for calculating an attention weighted sequence,\n",
        "or one attention-head \"\"\"\n",
        "def self_attention(input_sequence):\n",
        "    attention_weighted_sequence = np.zeros(shape=input_sequence.shape)\n",
        "    for i, pivot_vector in enumerate(input_sequence):\n",
        "        scores = np.zeros(shape=(len(input_sequence),))\n",
        "        for j, vector in enumerate(input_sequence):\n",
        "            scores[j] = np.dot(pivot_vector, vector.T)  # Unnormalized attention-score\n",
        "        scores /= np.sqrt(input_sequence.shape[1])  # Scale\n",
        "        scores = softmax(scores)  # Normalize to (0 to 1)\n",
        "        new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
        "        for j, vector in enumerate(input_sequence):\n",
        "            # Sum of all tokens weighted by the attention scores\n",
        "            new_pivot_representation += vector * scores[j]\n",
        "        # The new sums become our output\n",
        "        attention_weighted_sequence[i] = new_pivot_representation\n",
        "    return attention_weighted_sequence\n",
        "```"
      ],
      "metadata": {
        "id": "yF2D25pn1A1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Generalized self-attention: the _query-key-value_ model**\n",
        "So far, we've only considered one input sequence. However, the Transformer architecture was originally developed for machine translation, where you have to deal with two input sequences: the source sequence you're currently translating, and the target sequence you're converting it to. A Transformer is a _sequence-to-sequence_ model: it was designed to convert one sequence into another.\n",
        "\n",
        "The self-attention mechanism performs the following:\n",
        "> `outputs = sum(values * pairwise_attention_scores(query, key))`\n",
        "\n",
        "\"For each token in the `query`, compute how much the token is related to every token in `key`, and use these scores to weight a sum of tokens from `values`.\"\n",
        "\n",
        "This terminology comes from seach engines and recommender systems. You can write a query sequence, and that query is matched to a sequence of keys by the engine. Then the engine ranks the `keys` by strength of match, or _relevance_, wrt to the `query` and returns the `value` associated with the top number of matches (binary matching system, where a _dog_ in the `query` gets a 1 in the `keys` if it is in the key sequence, and a _cat_ gets a 0 because the `keys` sequence is only for e.g. canines).\n",
        "\n",
        "- _Query_ – A reference sequence that describes something you're looking for.\n",
        "- _Keys_ – Each value is assigned a key that describes the value in a format that can be readily compared to a query.\n",
        "- _Values_ – A body of knowledge that you're trying to extract information from.\n",
        "\n",
        "In practice, the _keys_ and the _values_ are often the same sequence.\n",
        "##### **Multi-head attention**\n",
        "Multi-head attention is an extra tweak to the self-attention mechanism. The \"multi-head\" moniker refers to the fact that the output space of the self-attention layer gets factored into a set of independent subspaces, learned separately: the intial query, key, and value sequences are sent through three _attention-heads_ each (one _head_ is the pseudocode we have above), resulting in three attention weighted vectors. This process is similar to having multiple kernels/filters in convolutional networks producing multiple feature maps.\n",
        "\n",
        "Having independent heads helps the layer learn different groups of features for each token, where features within one group are correlated with each other but are mostly independent from features in a different group."
      ],
      "metadata": {
        "id": "xP8Ny_Oh2kPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **The Transformer encoder**\n",
        "The Transformer encoder chains a multi-head attention layer with a dense projection and adds normalization as well as residual connections.\n",
        "\n",
        "The original Transformer architecture consists of two parts: an _encoder_ that processes the source sequence, and a _decoder_ that uses the source sequence to generate a translated version.\n",
        "\n",
        "The _encoder_ is a very generic module that ingests a sequence and learns to turn it into a more useful representation.\n",
        "\n",
        "Self-attention is a set-processing mechanism, focused on the relationships between pairs of sequence elements – it's blind to whether these elements occur at the beginning, at the end, or in the middle of a sequence. Why is a Transformer called a sequence model then?\n",
        "\n",
        "Transformer is a hybrid approach between a _bag-of-words_ and a _sequence_ model: order information is injected into the architecture with _positional encoding_.\n",
        "\n",
        "**POSITIONAL ENCODING**\n",
        "\n",
        "The idea behind positional encoding is very simple: to give the model access to word-order information, we're going to add the word's position in the sentence to each word embedding. The input word embeddings have two components:\n",
        "- _Word vector_ – Represents the word independently of any specific context\n",
        "- _Position vector_ – Represents the position of the word in the current sentence.\n",
        "\n",
        "One way to create positional vectors is through cosine functions, that map all values between -1 and 1. Another way is to learn position-embedding vectors the same way you would learn the word embedding vectors. The latter is called _positional embedding_."
      ],
      "metadata": {
        "id": "vi2K-D5XAGyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond text classification: sequence-to-sequence learning\n",
        "A sequence-to-sequence model takes a sequence as input and translates it into a different sequence. This is at the heart of many of the most successful applications in NLP:\n",
        "- _Machine translation_ – Convert a paragraph in a source langauge to its equivalent in a target language.\n",
        "- _Text summarization_ – Convert a long document ot a shorter version that retains the most important information.\n",
        "- _Text generation_ – Convert a text prompt into a paragraph that completes the prompt.\n",
        "\n",
        "The general template behind sequence-to-sequence models is two-fold. First during training,\n",
        "- An _encoder_ model turns the source sequence into an intermediate representation.\n",
        "- A _decoder_ is trained to predict the next token `i` in the target sequence by looking at both previous tokens and the encoded source sequence.\n",
        "\n",
        "During inference, we don't have access to the target sequence – we're trying to predict it from scratch. We'll have to generate it one token at a time:\n",
        "1. We obtain the encoded source sequence from the encoder.\n",
        "2. The decoder starts by looking at the encoded source sequence as well as an initial \"seed\" token, and used them to predict the first real token in the sequence.\n",
        "3. The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token.\n",
        "\n",
        "Sequence-to-sequence learning is the task where Transformer really shines. Neural attention enables Transformer models to successfully process sequences that are considerably longer and more complex than those RNNs can handle."
      ],
      "metadata": {
        "id": "PNZI9PVEk7JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "- There are two kinds of NLP model: _bag-of-words_ models that process sets of words or N-grams without taking into account their order, and _sequence_ models that process word order. A bag-of-words model is made of `Dense` layers, while a sequence model could be an RNN, a 1D convnet, or a Transformer.\n",
        "- _Word embeddings_ are vector spaces where semantic relationships between words are modeled as distance relationships between vectors that represent those words.\n",
        "- _Sequence-to-sequence learning_ is a generic, powerful learning framework that can be applied to solve many NLP problems, including machine translation. A sequence-to-sequence model is made of an encoder, which processes a source sequence, and a decoder, which tries to predict future tokens in target sequence by looking at past tokens, with the help of the encode-processed source sequence.\n",
        "- _Neural attention_ is a way to create context-aware word representations. It's the basis for the Transformer architecture.\n",
        "- The Transformer architecture, which consists of a `TransformerEncoder` and a `TransformerDecoder`, yields excellent results on sequence-to-sequence tasks. The `TransformerEncoder` can also be used for text classification or any sort of single-input NLP task."
      ],
      "metadata": {
        "id": "i1FtjVnxUgmW"
      }
    }
  ]
}